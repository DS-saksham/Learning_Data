{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e0fcad3-2f5d-45b4-86fc-21e630ff6f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The general idea of Gredient Descent is to tweak the parameters iteratively in order to minimize the cost function\n",
    "# You give theta a randome value (random initialization) then you improve it gradually taking \n",
    "# baby step a time , each step attempting to decrease the cost function(MSE) until the algorithm \n",
    "# converges to a minimum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7a0924-1157-47ef-8f23-19b8d8b0cb2d",
   "metadata": {},
   "source": [
    "##### 3 Technically speaking, its derivative is Lipschitz continuous.4 Since feature 1 is smaller, it takes a larger change in θ1 to affect the cost function, which is why the bowl is elongated along the θ1 axis.Figure 4-6. Gradient Descent pitfallsFortunately, the MSE cost function for a Linear Regression model happens to be aconvex function, which means that if you pick any two points on the curve, the line segment joining them never crosses the curve. This implies that there are no local minima, just one global minimum. It is also a continuous function with a slope thatnever changes abruptly. These two facts have a great consequence: Gradient Descent is guaranteed to approach arbitrarily close the global minimum (if you wait long enough and if the learning rate is not too high)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69c0a814-9ce0-4b4c-999c-9df76b7037b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When using Gradient Descent you should ensure that all features have a similar scale, or else it will take a much longer time to converge\n",
    "# The cost function is convex in  the case of Linear Regression \n",
    "# Since therer is only one golbal minimum grediet descent is guaranteed to find it as long as the learing reat is good\n",
    "# Unlike neural networks or non-linear models you wont get stuck in a bad local minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da44219-aff7-47b1-8c0a-b69d43fe1ce9",
   "metadata": {},
   "source": [
    "## Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cceaa9d3-da54-43fd-84e5-59a664e04a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To implements GD,you need to compute the gradient of the cost function with regard to each model parameters.\n",
    "# Batch GD uses the whole batch of training data at every step. GD  scales well with the number\n",
    "# of features trainig a Linear Regression model when there are hundreds of thousands of features is much\n",
    "# faster using Gradient Descent than using the Normal Equation or SVD decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd31c292-3c38-408b-adf2-f24edc7b930a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_b' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-51ac7a64a4bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mX_b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_b' is not defined"
     ]
    }
   ],
   "source": [
    "# Quick implementaion of GD\n",
    "import numpy as np\n",
    "eta = 0.1 # learning rate\n",
    "n_iterations = 20\n",
    "m = 100\n",
    "theta = np.random.rand(2,1)\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m*X_b.T.dot(X_b.dot(theta) -y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97f887c4-236f-456b-87e0-be8c074c3fa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.59235876],\n",
       "       [0.14900536]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc708e27-930d-41e1-83ef-e955206136fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
